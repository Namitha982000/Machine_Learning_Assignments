{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fe6c19-dbc6-4ed7-b9e3-6d70f94032e7",
   "metadata": {},
   "source": [
    "# Assignment 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76e1fc-136a-4bab-8d64-209d8f94d566",
   "metadata": {},
   "source": [
    "**1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdab333-c35a-4bb7-a6e2-a5634f9586a4",
   "metadata": {},
   "source": [
    "In decision trees, the Gini impurity is a measure of the node's impurity or uncertainty. The Gini impurity of a node is typically lower than or equal to that of its parent node, but it is not always guaranteed to be lower.\n",
    "\n",
    "When a decision tree algorithm constructs a tree, it aims to minimize the impurity at each node by selecting splits that maximize the reduction in impurity. This means that, in general, the child nodes resulting from a split will have a lower impurity than their parent node.\n",
    "\n",
    "However, it is possible for a split to result in child nodes with the same impurity as the parent node. This can happen if the split does not effectively separate the classes or if the impurity measure does not change after the split.\n",
    "\n",
    "In cases where the impurity measure used (such as Gini impurity or entropy) is not sensitive to the split or the classes are evenly distributed across the child nodes, the impurity of the child nodes can be equal to that of the parent node.\n",
    "\n",
    "Therefore, while the Gini impurity of a node is usually lower than its parent, it is not always the case. The exact relationship between the Gini impurity of a node and its parent depends on the specific dataset, the splits chosen, and the impurity measure being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a784f8a-a5d1-4d2e-bfef-39a63f4d7c85",
   "metadata": {},
   "source": [
    "**2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd34b0-fff2-44be-98d7-107be581d84b",
   "metadata": {},
   "source": [
    "In decision trees, the Gini impurity is a measure of the node's impurity or uncertainty. The Gini impurity of a node is typically lower than or equal to that of its parent node, but it is not always guaranteed to be lower.\n",
    "\n",
    "When a decision tree algorithm constructs a tree, it aims to minimize the impurity at each node by selecting splits that maximize the reduction in impurity. This means that, in general, the child nodes resulting from a split will have a lower impurity than their parent node.\n",
    "\n",
    "However, it is possible for a split to result in child nodes with the same impurity as the parent node. This can happen if the split does not effectively separate the classes or if the impurity measure does not change after the split.\n",
    "\n",
    "In cases where the impurity measure used (such as Gini impurity or entropy) is not sensitive to the split or the classes are evenly distributed across the child nodes, the impurity of the child nodes can be equal to that of the parent node.\n",
    "\n",
    "Therefore, while the Gini impurity of a node is usually lower than its parent, it is not always the case. The exact relationship between the Gini impurity of a node and its parent depends on the specific dataset, the splits chosen, and the impurity measure being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2b3bc-095b-400f-887b-6fb1ea5bd3c9",
   "metadata": {},
   "source": [
    "**3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4746985-bea9-47c2-a9e0-234d1bbc0a0d",
   "metadata": {},
   "source": [
    "If a decision tree is overfitting the training set, it may be wise to reduce its maximum depth. When a decision tree is overfitted, it performs poorly when generalising to new data because it collects noise or unimportant patterns in the training data. By restricting the tree's complexity and encouraging better generalisation, lowering the maximum depth aids in preventing overfitting.\n",
    "\n",
    "Here are a few reasons why reducing the maximum depth can help mitigate overfitting:\n",
    "\n",
    "1. Simplicity: A decision tree with a smaller depth is simpler and has fewer branches and nodes. By reducing the complexity of the tree, it becomes less likely to memorize noise or irrelevant details in the training data.\n",
    "\n",
    "2. Generalization: A shallower decision tree is less likely to fit the training data too closely, allowing it to capture more general patterns and relationships. This improves the tree's ability to generalize well to unseen data.\n",
    "\n",
    "3. Avoiding Over-Branching: If the maximum depth is not limited, a decision tree can potentially grow deep enough to perfectly fit or memorize every training instance. However, this can lead to poor performance on new data as the tree becomes too specialized to the training set.\n",
    "\n",
    "It's important to note that reducing the maximum depth is just one approach to combat overfitting in decision trees. Other techniques include limiting the minimum number of samples required to split a node, pruning the tree after training, or using ensemble methods like Random Forests that combine multiple decision trees.\n",
    "\n",
    "It's advisable to evaluate the model's performance on a separate validation or test set to determine the optimal maximum depth or other hyperparameters that strike a balance between fitting the training data and generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072854fd-50da-41df-84f0-d60c56361e71",
   "metadata": {},
   "source": [
    "**4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322c50e-d1cb-4570-a50e-c5d1f1ba4cee",
   "metadata": {},
   "source": [
    "For decision trees, scaling the input features is typically neither essential nor advantageous, even if the tree is underfitting the training set. Because they produce splits based on the relative ordering of the feature values rather than their absolute values, decision trees are not affected by the scale of the input features.\n",
    "\n",
    "Here are a few reasons why scaling the input features may not be a good idea for decision trees:\n",
    "\n",
    "1. Invariant to Monotonic Transformations: Decision trees are invariant to monotonic transformations of the input features. Scaling the features by a constant factor or applying any monotonic transformation (e.g., logarithm, square root) does not change the tree's structure or the splits it makes. Therefore, scaling the features will not impact the underfitting issue.\n",
    "\n",
    "2. Preservation of Relative Ordering: Decision trees make splits based on the relative ordering of the feature values within each node. Scaling the features does not alter their relative ordering, so the tree's decisions will remain the same.\n",
    "\n",
    "3. Focus on Splitting Points: Decision trees focus on finding optimal splitting points in the feature space to separate the target classes. The absolute values of the features do not affect the decision-making process, as long as the relative ordering is preserved.\n",
    "\n",
    "Instead of scaling the features, addressing underfitting in decision trees usually involves adjusting hyperparameters that control the tree's growth, such as increasing the maximum depth, reducing the minimum number of samples required to split a node, or using ensemble methods like Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77754548-4ee9-452c-b1fb-2e761737b08c",
   "metadata": {},
   "source": [
    "**5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8f5018-5b59-4875-b549-b965de756d3d",
   "metadata": {},
   "source": [
    "The number of instances, the amount of features, the complexity of the data, the hardware resources available, and the implementation efficiency all commonly affect how long it takes to train a decision tree.\n",
    "\n",
    "Given that it takes an hour to train a Decision Tree on a training set with 1 million instances, we can estimate the time required to train another Decision Tree on a training set of 10 million instances using a rough approximation:\n",
    "\n",
    "Assuming that the training time scales linearly with the number of instances, we can use the concept of proportionality:\n",
    "\n",
    "Training time for 10 million instances = (Training time for 1 million instances) * (Number of instances for 10 million / Number of instances for 1 million)\n",
    "\n",
    "Training time for 10 million instances = 1 hour * (10 million / 1 million)\n",
    "\n",
    "Training time for 10 million instances = 1 hour * 10\n",
    "\n",
    "Training time for 10 million instances = 10 hours\n",
    "\n",
    "Based on this rough approximation, it would take around 10 hours to train a Decision Tree on a training set with 10 million instances, assuming the training time scales linearly with the number of instances. However, please note that this is a rough estimate, and the actual training time may vary based on the factors mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c46598-fb78-48fd-81bc-0792bcdbbe04",
   "metadata": {},
   "source": [
    "**6. Will setting presort=True speed up training if your training set has 100,000 instances?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb3795-d5d5-4400-9269-46da234dd108",
   "metadata": {},
   "source": [
    "Setting `presort=True` in the context of training a Decision Tree can be used to speed up training in certain scenarios. However, whether it will actually speed up training with a training set of 100,000 instances depends on several factors.\n",
    "\n",
    "By setting `presort=True`, the algorithm will pre-sort the data based on the feature values before considering each split during training. This can save time during training when the dataset is relatively small or when the number of features is relatively large.\n",
    "\n",
    "However, for larger datasets, such as the one with 100,000 instances in your case, the pre-sorting process can become computationally expensive and potentially negate the speed benefits. The additional time required to pre-sort the data may outweigh the time saved during the training process.\n",
    "\n",
    "It's worth noting that the decision to set `presort=True` should be based on empirical testing and consideration of the specific characteristics of your dataset. Here are a few factors to consider:\n",
    "\n",
    "1. Dataset Size: As the dataset size increases, the pre-sorting process becomes more time-consuming. If the dataset is very large, it is generally more efficient to let the algorithm determine the optimal splitting points without pre-sorting.\n",
    "\n",
    "2. Number of Features: If the number of features is relatively large, pre-sorting can be more beneficial as it reduces the search space for finding the optimal split points.\n",
    "\n",
    "3. Hardware Resources: The computational resources available, such as CPU speed and memory capacity, can also impact the efficiency of pre-sorting. If your hardware has sufficient resources, the impact of pre-sorting may be less significant.\n",
    "\n",
    "In general, for a training set with 100,000 instances, it is advisable to conduct experiments by comparing the training times with and without `presort=True` to determine the actual impact on training speed. This empirical testing will provide more accurate insights into whether setting `presort=True` is beneficial in your specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef84302-3efd-487a-aac5-97c34b6822cd",
   "metadata": {},
   "source": [
    "**7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:**\n",
    "\n",
    "**a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).**\n",
    "\n",
    "**b. Divide the dataset into a training and a test collection with train test split().**\n",
    "\n",
    "**c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.**\n",
    "\n",
    "**d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643eb7b6-47c4-407b-875b-6bdb0022605f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_leaf_nodes': 10}\n",
      "Best Score: 0.850375\n",
      "Accuracy: 0.863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 10, 100, 500]\n",
    "}\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(tree_clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "best_tree_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "best_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = best_tree_clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1988a-c4eb-4a88-8797-3a3111a0ceba",
   "metadata": {},
   "source": [
    "**8. Follow these steps to grow a forest:**\n",
    "\n",
    "**a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.**\n",
    "\n",
    "**b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.**\n",
    "\n",
    "**c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.**\n",
    "\n",
    "**d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90ce3a3-29de-4dd7-aded-0bf9c1b1e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2585/1731652171.py:23: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions, _ = mode(predictions, axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.base import clone\n",
    "\n",
    "subsets = []\n",
    "shuffle_split = ShuffleSplit(n_splits=1000, test_size=100, random_state=42)\n",
    "for train_index, _ in shuffle_split.split(X_train):\n",
    "    subsets.append((X_train[train_index], y_train[train_index]))\n",
    "\n",
    "trees = []\n",
    "for subset in subsets:\n",
    "    tree_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "    tree_clf.fit(subset[0], subset[1])\n",
    "    trees.append(tree_clf)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "predictions = np.empty((len(X_test), len(trees)), dtype=np.uint8)\n",
    "for i, tree_clf in enumerate(trees):\n",
    "    predictions[:, i] = tree_clf.predict(X_test)\n",
    "\n",
    "majority_vote_predictions, _ = mode(predictions, axis=1)\n",
    "\n",
    "accuracy = np.mean(majority_vote_predictions.ravel() == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ca5b9-4039-462b-acca-464a7ef1ef64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1520a17-7df8-4980-8bc4-9f22b7155fcf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
