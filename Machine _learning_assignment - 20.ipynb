{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d229f3-f858-4fb0-af2a-3ea424d9ecee",
   "metadata": {},
   "source": [
    "# Assignment 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc7ac8-19b9-413b-803c-565786d2b46e",
   "metadata": {},
   "source": [
    "**1. What is the underlying concept of Support Vector Machines?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e01b2-4144-4650-a386-ea8b26c8d33b",
   "metadata": {},
   "source": [
    "The underlying concept of Support Vector Machines (SVM) is to find a hyperplane that best separates data points belonging to different classes in a classification problem. The goal is to find an optimal decision boundary that maximizes the margin between the classes, which leads to better generalization and improved classification performance.\n",
    "\n",
    "The key concepts in SVM are as follows:\n",
    "\n",
    "1. Hyperplane: In SVM, a hyperplane is a decision boundary that separates the data points into different classes. For example, in a two-dimensional space, a hyperplane is a line, while in a higher-dimensional space, it is a linear subspace.\n",
    "\n",
    "2. Margin: The margin is the region surrounding the hyperplane that separates the nearest data points of different classes. SVM aims to find a hyperplane with the maximum margin between classes, as this is associated with better generalization and reduced overfitting.\n",
    "\n",
    "3. Support Vectors: Support vectors are the data points that lie closest to the decision boundary. These points play a crucial role in defining the hyperplane and determining the margin. SVM derives its name from these support vectors.\n",
    "\n",
    "4. Kernel Trick: SVM can be extended to handle nonlinear decision boundaries by using the kernel trick. The kernel function allows mapping the original data into a higher-dimensional feature space, where a linear decision boundary can be applied. This avoids the explicit computation of the transformed features and makes SVM computationally efficient.\n",
    "\n",
    "5. Optimization: The training of an SVM involves solving an optimization problem to find the optimal hyperplane. The objective is to maximize the margin while minimizing the classification error. This optimization problem is typically solved using techniques like quadratic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e827d-f4de-4df5-bd51-6661f96c9ddb",
   "metadata": {},
   "source": [
    "**2. What is the concept of a support vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a33a13-d67f-437b-8c37-3c8791e48ef6",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM), a support vector is a data point that lies closest to the decision boundary (hyperplane) that separates different classes. These data points are critical for defining the decision boundary and determining the maximum margin in SVM.\n",
    "\n",
    "The support vectors are called so because they \"support\" or influence the construction of the decision boundary. They are the data points that have the most impact on the determination of the optimal hyperplane. In other words, if these support vectors were moved or removed, the position of the decision boundary would change.\n",
    "\n",
    "Support vectors are selected based on their proximity to the decision boundary. Only the support vectors that are within or on the margin contribute to the definition of the hyperplane. These support vectors lie on or close to the margin and are crucial for determining the maximum margin and achieving good generalization.\n",
    "\n",
    "During the training phase of SVM, the algorithm identifies the support vectors by finding the data points that are closest to the decision boundary. These support vectors are typically a subset of the original training data set. Once the support vectors are identified, they are used to compute the parameters of the decision boundary and determine the optimal hyperplane.\n",
    "\n",
    "Support vectors play a fundamental role in SVM, as they capture the essential information needed to classify new data points. They represent the critical samples that have the most influence on the separation of classes and contribute to the robustness and generalization capabilities of SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39caad3d-350e-4b12-abd9-970bedededd4",
   "metadata": {},
   "source": [
    "**3. When using SVMs, why is it necessary to scale the inputs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179905b-9dcb-4ddb-8af1-feb4ba81be6c",
   "metadata": {},
   "source": [
    "Scaling the inputs is necessary when using Support Vector Machines (SVMs) to ensure that all features contribute equally to the training process. There are a few reasons why scaling is important in SVMs:\n",
    "\n",
    "1. Influence of feature scales: SVMs use distance-based metrics to determine the optimal decision boundary. If the features have different scales or units, features with larger scales can dominate the optimization process. This can lead to biased results, where features with smaller scales have little impact on the decision boundary. Scaling the inputs ensures that all features are on a similar scale and have equal influence in determining the optimal decision boundary.\n",
    "\n",
    "2. Numerical stability: SVM algorithms involve solving optimization problems and working with numerical computations. Scaling the inputs helps improve numerical stability and convergence of the optimization process. Unbalanced feature scales can lead to numerical instabilities, such as overflow or underflow issues, that can affect the accuracy and performance of the SVM algorithm.\n",
    "\n",
    "3. Regularization parameter interpretation: In SVMs, there is a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing the classification errors. The choice of C affects the tolerance for misclassification. Scaling the inputs ensures that the regularization parameter is applied consistently across all features, making it easier to interpret and set an appropriate value for C.\n",
    "\n",
    "4. Kernel functions: When using kernel functions in SVMs, such as the Radial Basis Function (RBF) kernel, scaling becomes even more important. Kernel functions rely on the computation of distances between data points, and unbalanced feature scales can lead to biased results. Scaling the inputs ensures that the kernel function operates effectively and captures meaningful patterns in the data.\n",
    "\n",
    "By scaling the inputs, the features are brought to a common scale, making the SVM algorithm more robust, accurate, and efficient. Common scaling techniques include standardization (mean centering and scaling to unit variance) or normalization (scaling features to a specific range). The choice of scaling technique depends on the nature of the data and the specific requirements of the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116754c5-0ff9-4455-adce-20923f9c1aa1",
   "metadata": {},
   "source": [
    "**4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8534e29-c578-4387-a290-66ea048e4abd",
   "metadata": {},
   "source": [
    "In general, SVM classifiers do not directly provide confidence scores or percentage chances for their predictions. Unlike some other classifiers, such as probabilistic classifiers like logistic regression or Naive Bayes, SVMs do not inherently provide a probability estimation.\n",
    "\n",
    "SVM classifiers are binary classifiers that aim to find an optimal decision boundary to separate data points belonging to different classes. The prediction output of an SVM classifier is based on which side of the decision boundary a data point falls into.\n",
    "\n",
    "However, there are approaches to obtain confidence scores or probability estimates from SVM classifiers. Two common methods are:\n",
    "\n",
    "1. Platt Scaling: Platt scaling is a technique used to calibrate the outputs of an SVM classifier into probability estimates. It involves training a separate logistic regression model on the SVM outputs using a labeled validation set. The logistic regression model is trained to map the SVM outputs to calibrated probabilities. This approach allows obtaining confidence scores or probability estimates from the SVM classifier.\n",
    "\n",
    "2. Probability Estimates from Support Vector Machines (PESVM): PESVM is an extension to the original SVM formulation that incorporates probability estimates. It directly models probability estimates within the SVM training process. PESVM modifies the SVM optimization problem to directly optimize for probability estimates instead of margin maximization. This approach provides probability estimates as an output from the SVM classifier without the need for additional calibration.\n",
    "\n",
    "Both Platt scaling and PESVM provide a way to obtain confidence scores or probability estimates from an SVM classifier. These approaches allow for a measure of confidence in the predictions, indicating the likelihood of a data point belonging to a particular class. However, it's important to note that these methods introduce additional complexity and may require additional computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3b3e6-2f10-41c4-a035-7b25ace2f3a1",
   "metadata": {},
   "source": [
    "**5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beba789-eb63-49dc-9b0d-d2ae1959f51e",
   "metadata": {},
   "source": [
    "When training a model on a training set with millions of instances and hundreds of features, it is generally more efficient to use the dual form of the SVM problem rather than the primal form.\n",
    "\n",
    "The dual form of the SVM problem is preferred in scenarios where the number of training instances (samples) is much larger than the number of features. This is often the case in high-dimensional datasets with a large number of instances, such as in text classification or image recognition tasks.\n",
    "\n",
    "The main advantage of using the dual form in such scenarios is that it allows for better computational efficiency. The dual form involves solving a quadratic optimization problem with respect to the Lagrange multipliers associated with the training instances. This formulation benefits from the kernel trick, which allows for efficient computation of the inner products between pairs of training instances without explicitly calculating the high-dimensional feature space.\n",
    "\n",
    "On the other hand, the primal form of the SVM problem involves optimizing the decision function directly in terms of the model parameters, which can be computationally expensive when dealing with high-dimensional datasets.\n",
    "\n",
    "By using the dual form, the computational burden is shifted from dealing with the large number of features to handling the pairwise interactions between the training instances. This can significantly reduce the complexity and computational requirements of training the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d0d5a-1c93-4d1c-8166-30758a44ea2f",
   "metadata": {},
   "source": [
    "**6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce659cac-159e-4f3d-b92e-4b24ed64220a",
   "metadata": {},
   "source": [
    "If you have trained an SVM classifier with an RBF kernel and it appears to underfit the training data, you can try adjusting the hyperparameters, specifically gamma and C, to improve the model's performance.\n",
    "\n",
    "1. Gamma (γ): Gamma is a parameter of the RBF kernel that determines the influence of individual training samples. It controls the flexibility of the decision boundary. When gamma is low, the influence of a single training example is more widespread, resulting in a smoother decision boundary. Conversely, when gamma is high, the influence is localized to nearby training examples, leading to a more complex and tighter decision boundary.\n",
    "\n",
    "If the SVM classifier is underfitting the training data, meaning it is not capturing the underlying patterns and is too simplistic, you can try increasing the value of gamma. This makes the model more sensitive to individual data points and can potentially improve its ability to capture complex relationships in the data.\n",
    "\n",
    "2. C: C is the regularization parameter in SVM that controls the trade-off between achieving a low training error and a wider margin. It balances the fitting of the data points versus the regularization term. A smaller value of C allows for a wider margin but may lead to more misclassified training samples, while a larger value of C enforces a smaller margin and aims to correctly classify more training samples.\n",
    "\n",
    "If the SVM classifier is underfitting, you can try decreasing the value of C. This allows for a wider margin and increases the tolerance for misclassification, potentially leading to a more flexible decision boundary that can better capture the training data's underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7fc1c-3bd5-4fb1-96e7-1f7f1aa64c14",
   "metadata": {},
   "source": [
    "**7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751bd544-528d-4fe4-bbbd-747face8f35a",
   "metadata": {},
   "source": [
    "To solve the soft margin linear SVM classifier problem using an off-the-shelf quadratic programming (QP) solver, you need to properly set the QP parameters: H, f, A, and b. Here's how these parameters are defined:\n",
    "\n",
    "1. H: The Hessian matrix is defined as H = (X^T)(X), where X is the matrix of feature vectors of the training instances. H is a positive semi-definite matrix.\n",
    "\n",
    "2. f: The linear coefficient vector f is defined as f = -1 * (1, 1, ..., 1), where the length of f is equal to the number of training instances. Each element of f corresponds to a penalty term for misclassification.\n",
    "\n",
    "3. A: The constraint matrix A is defined based on the slack variables and the class labels. It depends on the specific formulation of the soft margin SVM problem. Each row of A represents a linear constraint.\n",
    "\n",
    "4. b: The constraint vector b is defined based on the class labels and the C parameter. It also depends on the specific formulation of the soft margin SVM problem. Each element of b corresponds to a threshold value for the constraints.\n",
    "\n",
    "The specific values of H, f, A, and b depend on the dataset and the formulation of the soft margin SVM problem. They need to be set according to the constraints and objective function of the problem.\n",
    "\n",
    "Once you have defined the QP parameters, you can pass them to the off-the-shelf QP solver, which will solve the quadratic programming problem and provide the optimal solution, which includes the coefficients of the decision function and the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f399f-b45c-494b-88cc-4031f571ad96",
   "metadata": {},
   "source": [
    "**8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a404dc46-bf11-4acd-be5b-280dc2230a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Accuracy: 1.0\n",
      "SVC Accuracy: 0.95\n",
      "SGDClassifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a linearly separable dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LinearSVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Train SVC\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Train SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge', max_iter=1000, random_state=42)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "linear_svc_pred = linear_svc.predict(X_test)\n",
    "svc_pred = svc.predict(X_test)\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "# Compare the accuracies of the classifiers\n",
    "linear_svc_accuracy = accuracy_score(y_test, linear_svc_pred)\n",
    "svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_pred)\n",
    "\n",
    "print(\"LinearSVC Accuracy:\", linear_svc_accuracy)\n",
    "print(\"SVC Accuracy:\", svc_accuracy)\n",
    "print(\"SGDClassifier Accuracy:\", sgd_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda17ad-4711-44aa-a612-2e7e9fdc5d49",
   "metadata": {},
   "source": [
    "**9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e32d3-8977-4d75-a874-fcf038e22cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2163/1410228034.py:18: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  y_subset = y_train[:subset_size]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Subset the data for hyperparameter tuning\n",
    "subset_size = 10000\n",
    "X_subset = X_train[:subset_size]\n",
    "y_subset = y_train[:subset_size]\n",
    "\n",
    "# Define the SVM classifier with OvR strategy\n",
    "svm = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1]}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_subset, y_subset)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the SVM classifier with the best hyperparameters\n",
    "svm = SVC(kernel='rbf', decision_function_shape='ovr', **best_params)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30aa594-6197-47d6-ab6c-c1c56fec46fa",
   "metadata": {},
   "source": [
    "**10. On the California housing dataset, train an SVM regressor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f464cde0-eabe-42eb-813f-782e2993e2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3320115421348744\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the SVM regressor\n",
    "svm_regressor = SVR(kernel='rbf')\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_regressor.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44941d-6ed3-4c5e-9560-4e6c0becaebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
