{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc196f7-5d9e-4993-b469-f3675e37865b",
   "metadata": {},
   "source": [
    "**1. Provide an example of the concepts of Prior, Posterior, and Likelihood.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b1b12-1892-42f6-940c-6540f56f5037",
   "metadata": {},
   "source": [
    "Yes Let's consider a simple example of a medical test for a rare disease. \n",
    "Assume the prevalence of the disease in the general population is low, about 1 in 10,000 people. Now, a patient undergoes a diagnostic test for this disease, and the test result comes back positive.\n",
    "\n",
    "In this scenario:\n",
    "\n",
    "Prior probability: The prior probability is the initial belief or probability assigned to an event before considering any specific evidence. Here, the prior probability would be the prevalence of the disease in the general population, which is 1 in 10,000 or 0.0001.\n",
    "\n",
    "Posterior probability: The posterior probability is the updated probability of an event or hypothesis after considering specific evidence. In this case, the posterior probability would be the updated probability of the patient having the disease after receiving the positive test result. It combines the prior probability with the likelihood of the test result given the presence or absence of the disease.\n",
    "\n",
    "Likelihood: The likelihood probability represents the probability of obtaining a specific test result given the presence or absence of the disease. In this example, the likelihood would be the probability of obtaining a positive test result if the patient actually has the disease. Let's assume the test has a sensitivity of 95%, meaning it correctly detects the disease 95% of the time.\n",
    "\n",
    "Using Bayes' theorem, we can calculate the posterior probability by combining the prior probability and the likelihood:\n",
    "\n",
    "Posterior Probability = (Prior Probability * Likelihood) / Evidence\n",
    "\n",
    "In this case, the evidence is the probability of obtaining a positive test result, which can be calculated by considering both true positive and false positive rates.\n",
    "\n",
    "Let's assume the specificity of the test is 90%, meaning it correctly identifies the absence of the disease 90% of the time. Therefore, the evidence would be:\n",
    "\n",
    "Evidence = (Prior Probability * Likelihood) + ((1 - Prior Probability) * (1 - Specificity))\n",
    "\n",
    "By plugging in the numbers, we can calculate the posterior probability, which represents the updated probability of the patient having the disease given the positive test result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b61021-60a0-42cd-9af3-7dc68a92f050",
   "metadata": {},
   "source": [
    "**2. What role does Bayes' theorem play in the concept learning principle?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b8758-c232-498b-ac71-ae23014d636b",
   "metadata": {},
   "source": [
    "Bayes' theorem plays a fundamental role in the concept learning principle by providing a framework to update and revise beliefs or hypotheses based on new evidence. The concept learning principle is concerned with inferring or learning concepts from observed data or examples.\n",
    "\n",
    "In the context of concept learning, Bayes' theorem allows us to calculate the posterior probability of a hypothesis or concept given the observed data. It combines the prior probability of the hypothesis with the likelihood of observing the data given that hypothesis.\n",
    "\n",
    "The general form of Bayes' theorem is:\n",
    "\n",
    "Posterior Probability = (Prior Probability * Likelihood) / Evidence\n",
    "\n",
    "In concept learning, the posterior probability represents the updated belief or probability of a hypothesis being true after considering the observed data. The prior probability reflects the initial belief or probability assigned to the hypothesis before observing any data. The likelihood probability quantifies the agreement between the observed data and the hypothesis. And the evidence serves as a normalizing factor to ensure that the posterior probability is a valid probability distribution.\n",
    "\n",
    "The concept learning principle often involves iteratively applying Bayes' theorem to update the beliefs or probabilities of different hypotheses as more data is observed. It allows for a systematic and principled approach to refine and revise hypotheses based on the available evidence.\n",
    "\n",
    "By incorporating Bayes' theorem into the concept learning process, it provides a probabilistic framework to handle uncertainty and allows for a more robust and rational approach to hypothesis revision and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3701797-214d-4513-9361-df8935bceed5",
   "metadata": {},
   "source": [
    "**3. Offer an example of how the Nave Bayes classifier is used in real life.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db4867-c94f-4e43-a108-cc99c21d13d2",
   "metadata": {},
   "source": [
    "One practical example of how the Naïve Bayes classifier is used in real life is in email spam filtering. Spam filtering is the task of automatically identifying and filtering out unwanted or unsolicited email messages (spam) from legitimate ones (ham).\n",
    "\n",
    "In this application, the Naïve Bayes classifier is trained using a dataset of labeled emails, where each email is classified as spam or ham. The classifier learns the probabilities of certain words or features occurring in spam and ham emails.\n",
    "\n",
    "During training, the Naïve Bayes classifier calculates the prior probabilities of spam and ham based on their frequencies in the training set. It also estimates the likelihood probabilities of specific words occurring in spam and ham emails based on their frequencies.\n",
    "\n",
    "To classify a new incoming email, the Naïve Bayes classifier calculates the posterior probabilities of spam and ham given the presence or absence of specific words in the email. It combines the prior probabilities and likelihood probabilities using Bayes' theorem.\n",
    "\n",
    "The email is then classified as spam or ham based on the class with the highest posterior probability. If the posterior probability of spam is higher, the email is classified as spam and can be filtered out or flagged accordingly.\n",
    "\n",
    "The Naïve Bayes classifier is well-suited for spam filtering due to its simplicity, efficiency, and ability to handle high-dimensional feature spaces. It can quickly process and classify large volumes of emails, making it a popular choice for real-time spam filtering in email services and clients. Additionally, Naïve Bayes classifiers often perform well in this context, achieving high accuracy rates in identifying spam while minimizing false positives (legitimate emails classified as spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36b3b7-0adb-41c9-830d-eed1ff7a82b7",
   "metadata": {},
   "source": [
    "**4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about doing it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531a69b-e841-4dae-8be2-f515f161dd13",
   "metadata": {},
   "source": [
    "Yes, the Naïve Bayes classifier can be used with continuous numeric data. However, it requires some additional steps to handle continuous variables since the Naïve Bayes classifier assumes independence between features.\n",
    "\n",
    "To use the Naïve Bayes classifier with continuous numeric data, you typically need to apply a probability distribution estimation technique to model the likelihood probabilities of the features given each class. Two common approaches are:\n",
    "\n",
    "1. Gaussian Naïve Bayes: In Gaussian Naïve Bayes, it is assumed that the features follow a Gaussian (normal) distribution. The mean and variance of each feature are estimated for each class in the training data. To classify a new sample, the classifier calculates the likelihood of each feature value using the corresponding Gaussian distribution parameters (mean and variance) for each class\n",
    ".\n",
    "2. Kernel Density Estimation (KDE): KDE is a non-parametric method that estimates the probability density function of the features given each class. It estimates the probability distribution of the features using kernel functions centered at each training data point. To classify a new sample, the classifier calculates the likelihood of the feature values based on their estimated densities for each class.\n",
    "\n",
    "Both of these approaches allow the Naïve Bayes classifier to handle continuous numeric data. However, it's important to note that the Naïve Bayes assumption of feature independence might not hold in practice, especially if there are correlations or dependencies between the continuous features. In such cases, more advanced techniques like Gaussian Mixture Models or Bayesian networks may be more suitable.\n",
    "\n",
    "Preprocessing steps like normalization or scaling of the continuous features can also be applied to ensure they have similar scales and reduce the impact of outliers on the classifier's performance.\n",
    "\n",
    "Overall, while the Naïve Bayes classifier can be used with continuous numeric data, careful consideration of the distributional assumptions and preprocessing techniques is necessary to achieve optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a4f91-e962-473a-9076-bdc47d358f7e",
   "metadata": {},
   "source": [
    "**5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they capable of resolving a wide range of issues?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3576d12-a593-4773-ab0b-301296e93cc0",
   "metadata": {},
   "source": [
    "Bayesian Belief Networks (BBNs), also known as Bayesian Networks or Probabilistic Graphical Models, are graphical models that represent probabilistic relationships among a set of variables. BBNs are based on Bayesian probability theory and provide a structured way to model and reason about uncertainty and dependencies between variables.\n",
    "\n",
    "In a BBN, variables are represented as nodes in a directed acyclic graph, and the relationships between variables are represented by directed edges or arcs. Each node in the graph corresponds to a random variable, and the edges represent conditional dependencies between the variables. The strength of the dependencies is quantified by conditional probability tables (CPTs), which specify the probabilities of a variable given its parents in the graph.\n",
    "\n",
    "BBNs work by utilizing Bayes' theorem and graphical inference algorithms to propagate probabilities and make probabilistic inferences. By observing evidence for some variables, the BBN can update the probabilities of other variables in the network using Bayesian inference. The network can be used to calculate the posterior probabilities of variables given observed evidence or to predict the probabilities of variables in the absence of evidence.\n",
    "\n",
    "Applications of Bayesian Belief Networks are diverse and can be found in various domains, including:\n",
    "\n",
    "1. Decision Support Systems: BBNs are used to model complex decision problems by incorporating probabilistic dependencies and uncertainties. They assist in decision-making by providing insights into the probabilities of different outcomes based on available evidence.\n",
    "\n",
    "2. Medical Diagnosis and Treatment: BBNs are applied in medical domains to aid in diagnosis and treatment decisions. They can model the relationships between symptoms, test results, and diseases, helping doctors make informed decisions based on available evidence.\n",
    "\n",
    "3. Risk Assessment and Management: BBNs are used for risk analysis and management in fields such as finance, engineering, and environmental sciences. They help assess and predict the likelihood of risks occurring and support decision-making for risk mitigation.\n",
    "\n",
    "4. Natural Language Processing: BBNs are utilized in language processing tasks such as text classification, information retrieval, and sentiment analysis. They enable probabilistic modeling of linguistic structures and assist in making inferences based on textual data.\n",
    "\n",
    "While BBNs are powerful tools for modeling uncertainty and dependencies, they have certain limitations. BBNs assume that the relationships between variables are static and do not account for feedback loops or dynamic changes. Moreover, building accurate BBNs often requires substantial domain expertise and sufficient data for parameter estimation. However, when appropriately constructed and trained, BBNs can effectively resolve a wide range of problems involving uncertain and probabilistic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589bc97-62b7-4527-904a-bf120efc154e",
   "metadata": {},
   "source": [
    "**6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98 and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered, implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) = 0.00001. What are the chances that an alarm would be triggered when an individual is actually an intruder?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef900a79-f3d8-4656-a5da-b4fa13b7650e",
   "metadata": {
    "tags": []
   },
   "source": [
    "To calculate the chances that an alarm would be triggered when an individual is actually an intruder, we need to use Bayes' theorem. Let's denote the event of an alarm being triggered as A and the event of an individual being an intruder as I. We want to find the probability P(I = 1|A = 1), which is the posterior probability of an individual being an intruder given that an alarm is triggered.\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "\n",
    "P(I = 1|A = 1) = (P(A = 1|I = 1) * P(I = 1)) / P(A = 1)\n",
    "\n",
    "Given the information provided:\n",
    "\n",
    "P(A = 1|I = 1) = 0.98 (probability of detecting an intruder)\n",
    "\n",
    "P(A = 1|I = 0) = 0.001 (probability of a false positive)\n",
    "\n",
    "P(I = 1) = 0.00001 (likelihood of an intruder)\n",
    "\n",
    "To calculate P(A = 1), we can use the law of total probability:\n",
    "\n",
    "P(A = 1) = P(A = 1|I = 1) * P(I = 1) + P(A = 1|I = 0) * P(I = 0)\n",
    "\n",
    "Since P(I = 0) = 1 - P(I = 1), we have:\n",
    "\n",
    "P(A = 1) = P(A = 1|I = 1) * P(I = 1) + P(A = 1|I = 0) * (1 - P(I = 1))\n",
    "\n",
    "Now, let's calculate P(A = 1):\n",
    "\n",
    "P(A = 1) = 0.98 * 0.00001 + 0.001 * (1 - 0.00001)\n",
    "         = 0.0000098 + 0.000999\n",
    "         = 0.0010088\n",
    "         \n",
    "Now we can calculate P(I = 1|A = 1) using Bayes' theorem:\n",
    "\n",
    "P(I = 1|A = 1) = (P(A = 1|I = 1) * P(I = 1)) / P(A = 1)\n",
    "               = (0.98 * 0.00001) / 0.0010088\n",
    "               ≈ 0.0097\n",
    "               \n",
    "Therefore, the chances that an alarm would be triggered when an individual is actually an intruder is approximately 0.0097 or about 0.97%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31fad0-bf18-49a7-bfb5-06f7f1e388f8",
   "metadata": {},
   "source": [
    "**7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune (random variable D).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4f6cf-20a7-4d90-8b63-f421cfc1d302",
   "metadata": {},
   "source": [
    "To calculate the likelihood that a person who tests positive is actually immune (antibiotic-resistant), we can use Bayes' theorem. Let's denote the event of a positive test result as T = 1 and the event of being immune (antibiotic-resistant) as D = 1. We want to find the probability P(D = 1|T = 1), which is the posterior probability of a person being immune given a positive test result.\n",
    "\n",
    "According to Bayes' theorem, we have:\n",
    "\n",
    "P(D = 1|T = 1) = (P(T = 1|D = 1) * P(D = 1)) / P(T = 1)\n",
    "\n",
    "Given the information provided:\n",
    "\n",
    "P(T = 1|D = 1) = 1 - 0.05 (probability of a positive test result given immunity)\n",
    "\n",
    "P(T = 1|D = 0) = 0.01 (probability of a positive test result given non-immunity)\n",
    "\n",
    "P(D = 1) = 0.02 (likelihood of being immune)\n",
    "\n",
    "To calculate P(T = 1), we can use the law of total probability:\n",
    "\n",
    "P(T = 1) = P(T = 1|D = 1) * P(D = 1) + P(T = 1|D = 0) * P(D = 0)\n",
    "\n",
    "Since P(D = 0) = 1 - P(D = 1), we have:\n",
    "\n",
    "P(T = 1) = (1 - 0.05) * 0.02 + 0.01 * (1 - 0.02)\n",
    "         = 0.0196\n",
    "         \n",
    "Now we can calculate P(D = 1|T = 1) using Bayes' theorem:\n",
    "\n",
    "P(D = 1|T = 1) = (P(T = 1|D = 1) * P(D = 1)) / P(T = 1)\n",
    "               = (1 - 0.05) * 0.02 / 0.0196\n",
    "               = 0.0196 / 0.0196\n",
    "               = 1\n",
    "               \n",
    "Therefore, the likelihood that a person who tests positive is actually immune (antibiotic-resistant) is 1, indicating a certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860f832-855d-4cd5-accf-ee46ec542a12",
   "metadata": {},
   "source": [
    "**8. In order to prepare for the test, a student knows that there will be one question in the exam that is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and 50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10 type B problems, and 6 of 10 type C problems.**\n",
    "\n",
    "**1. What is the likelihood that the student can solve the exam problem?**\n",
    "\n",
    "**2. Given the student's solution, what is the likelihood that the problem was of form A?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce6788-64f9-44c4-ab26-7cdbe531463b",
   "metadata": {},
   "source": [
    "To answer the given questions, we can use Bayes' theorem and calculate the conditional probabilities based on the information provided.\n",
    "\n",
    "1. To find the likelihood that the student can solve the exam problem, we need to consider the probabilities of each type of problem and the student's ability to solve them. Let's denote the event of the problem being of type A, B, or C as P(A), P(B), and P(C) respectively, and the event of the student solving the problem as S.\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(A) = 0.30 (probability of the problem being of type A)\n",
    "\n",
    "P(B) = 0.20 (probability of the problem being of type B)\n",
    "\n",
    "P(C) = 0.50 (probability of the problem being of type C)\n",
    "\n",
    "The student's ability to solve each type of problem:\n",
    "\n",
    "P(S|A) = 9/10 (probability of solving a type A problem)\n",
    "\n",
    "P(S|B) = 2/10 (probability of solving a type B problem)\n",
    "\n",
    "P(S|C) = 6/10 (probability of solving a type C problem)\n",
    "\n",
    "Using Bayes' theorem, we can calculate the likelihood of the student solving the exam problem:\n",
    "\n",
    "P(S) = P(A) * P(S|A) + P(B) * P(S|B) + P(C) * P(S|C)\n",
    "\n",
    "P(S) = 0.30 * (9/10) + 0.20 * (2/10) + 0.50 * (6/10)\n",
    "     = 0.27 + 0.04 + 0.30\n",
    "     = 0.61\n",
    "     \n",
    "Therefore, the likelihood that the student can solve the exam problem is 0.61 or 61%.\n",
    "\n",
    "2. To find the likelihood that the problem was of form A given the student's solution, we can use Bayes' theorem again. Let's denote the event of the problem being of form A as A and the event of the student solving the problem as S.\n",
    "\n",
    "We want to calculate:\n",
    "\n",
    "P(A|S) = (P(S|A) * P(A)) / P(S)\n",
    "\n",
    "Using the values from the previous calculations:\n",
    "\n",
    "P(S|A) = 9/10 (probability of solving a type A problem)\n",
    "\n",
    "P(A) = 0.30 (probability of the problem being of type A)\n",
    "\n",
    "P(S) = 0.61 (likelihood of the student solving the problem)\n",
    "P(A|S) = (9/10 * 0.30) / 0.61\n",
    "       = 0.27 / 0.61\n",
    "       ≈ 0.443\n",
    "       \n",
    "Therefore, the likelihood that the problem was of form A given the student's solution is approximately 0.443 or 44.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8ce2f-1d76-436b-93c6-b5f1e930ae56",
   "metadata": {},
   "source": [
    "**9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the camera can take a false photograph with a 10% chance of detecting movement from other objects.**\n",
    "\n",
    "**1. How many customers come into the bank on a daily basis (10 hours)?**\n",
    "\n",
    "**2. On a daily basis, how many fake photographs (photographs taken when there is no customer) and how many missed photographs (photographs taken when there is a customer) are there?**\n",
    "\n",
    "**3. Explain likelihood that there is a customer if there is a photograph?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4bcab9-edf2-4275-a401-181bfcaccf35",
   "metadata": {},
   "source": [
    "To answer the given questions, let's break them down step by step:\n",
    "\n",
    "1. To calculate the number of customers coming into the bank on a daily basis (10 hours), we need to determine the number of 5-minute time periods in 10 hours and then multiply it by the probability of a customer coming in each time period.\n",
    "\n",
    "10 hours = 10 * 60 minutes = 600 minutes\n",
    "\n",
    "Number of 5-minute time periods = 600 minutes / 5 minutes = 120 time periods\n",
    "\n",
    "Probability of a customer coming in each 5-minute time period = 5%\n",
    "\n",
    "Number of customers coming into the bank on a daily basis = 120 time periods * 5% = 6 customers\n",
    "\n",
    "Therefore, on a daily basis, approximately 6 customers come into the bank.\n",
    "\n",
    "2. On a daily basis (10 hours), we can calculate the number of fake photographs (photographs taken when there is no customer) and the number of missed photographs (photographs taken when there is a customer) based on the probabilities given.\n",
    "\n",
    "Probability of the CCTV taking a false photograph when there is no customer = 10%\n",
    "\n",
    "Probability of the CCTV missing a customer and not detecting them = 1 - 99% = 1%\n",
    "\n",
    "Number of fake photographs = Number of 5-minute time periods * Probability of a false photograph\n",
    "                        = 120 time periods * 10%\n",
    "                        = 12 photographs\n",
    "                        \n",
    "Number of missed photographs = Number of 5-minute time periods * Probability of missing a customer\n",
    "                          = 120 time periods * 1%\n",
    "                          = 1.2 photographs (approximated to 1 photograph)\n",
    "                          \n",
    "Therefore, on a daily basis, there are approximately 12 fake photographs and 1 missed photograph.\n",
    "\n",
    "3. To explain the likelihood that there is a customer if there is a photograph, we need to calculate the conditional probability using Bayes' theorem. Let's denote the event of a customer being present as C and the event of a photograph being taken as P.\n",
    "\n",
    "We want to find the probability P(C = 1|P = 1), which is the likelihood that there is a customer given that a photograph was taken.\n",
    "According to Bayes' theorem:\n",
    "\n",
    "P(C = 1|P = 1) = (P(P = 1|C = 1) * P(C = 1)) / P(P = 1)\n",
    "\n",
    "Given the information provided:\n",
    "\n",
    "P(P = 1|C = 1) = 99% (probability of the CCTV detecting a customer)\n",
    "\n",
    "P(P = 1|C = 0) = 10% (probability of a false photograph)\n",
    "\n",
    "P(C = 1) = 5% (probability of a customer coming in each time period)\n",
    "\n",
    "To calculate P(P = 1), we can use the law of total probability:\n",
    "\n",
    "P(P = 1) = P(P = 1|C = 1) * P(C = 1) + P(P = 1|C = 0) * P(C = 0)\n",
    "P(P = 1) = 99% * 5% + 10% * (1 - 5%)\n",
    "         = 0.0495 + 0.005\n",
    "         = 0.0545\n",
    "         \n",
    "Now we can calculate P(C = 1|P = 1) using Bayes' theorem:\n",
    "\n",
    "P(C = 1|P = 1) = (P(P = 1|C = 1) * P(C = 1)) / P(P = 1)\n",
    "               = (99% * 5%) / 0.0545\n",
    "               = 0.0495 / 0.0545\n",
    "               ≈ 0.907\n",
    "               \n",
    "Therefore, the likelihood that there is a customer if there is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbfabe-bf5d-48ae-9237-b4167430b251",
   "metadata": {},
   "source": [
    "**10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief network to represent the conditional independence assumptions of the Nave Bayes classifier for the match winning prediction problem in Section 6.4.4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6f023-8331-46ee-a69e-e35e658dfe38",
   "metadata": {},
   "source": [
    "To create the conditional probability table (CPT) associated with the node \"Won Toss\" in the Bayesian Belief Network for the match winning prediction problem, we need to make conditional independence assumptions based on the Naive Bayes classifier.\n",
    "Assuming that the \"Won Toss\" node is dependent on a single parent node representing the outcome of the \"Team Strength\" attribute, the CPT for \"Won Toss\" would look as follows:\n",
    "```\n",
    "+----------------------+---------------------+\n",
    "| Team Strength        | P(Won Toss = True)  |\n",
    "+----------------------+---------------------+\n",
    "| Weak                 | P(WT = True|WS = W) |\n",
    "+----------------------+---------------------+\n",
    "| Moderate             | P(WT = True|WS = M) |\n",
    "+----------------------+---------------------+\n",
    "| Strong               | P(WT = True|WS = S) |\n",
    "+----------------------+---------------------+\n",
    "```\n",
    "In the table, \"WS\" represents the \"Team Strength\" attribute, and the values 'W', 'M', and 'S' correspond to the possible levels of strength (Weak, Moderate, Strong).\n",
    "For each level of \"Team Strength,\" you would assign a conditional probability of the \"Won Toss\" node being true based on that strength level. These probabilities would need to be determined from the training data or domain knowledge.\n",
    "Note that the table represents the conditional independence assumption of the Naive Bayes classifier, assuming that the \"Won Toss\" node is conditionally independent of all other attributes given the \"Team Strength\" attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af0e28-36ff-4ea0-8eb7-1ba53bfab576",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
